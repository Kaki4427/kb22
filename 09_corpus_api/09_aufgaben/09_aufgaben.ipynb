{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgaben 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 (DTA-Crawler)\n",
    "Schreiben Sie ein Python-Programm, das verschiedene Korpusdateien vom\n",
    "DTA-Korpus über einen Query an die DTA-Web-API-Schnittstelle herunter in das aktuelle Verzeichnis lädt.  Für eine\n",
    "Suchanfrage sollen für maximal 10 *unterschiedliche* Dokumente\n",
    "jeweils die TCF- und TEI-Dateien heruntergeladen werden:\n",
    "\n",
    "* Das Programm erwartet einen Suchbegriff als einziges\n",
    "  Kommandozeilenargument\n",
    "* Über den Schalter `--max` soll die maximale Anzahl der Dokumente\n",
    "  festgelegt werden (Standardwert: 10)\n",
    "* Über den Schalter `--dir` soll das Ausgabeverzeichnis eingestellt\n",
    "  werden (Standardwert: `.`)\n",
    "* Achten Sie darauf nich zu viele Suchanfragen zu schnell an den\n",
    "  Server zu schicken!\n",
    "\n",
    "```bash\n",
    "$ ./dta-crawler --max 5 --dir corpus Axolotl\n",
    "downloading to corpus/brehm_thierleben_1869.tei.xml\n",
    "downloading to corpus/brehm_thierleben_1869.tcf.xml\n",
    "...\n",
    "$ ls corpus\n",
    "brehm_thierleben_1869.tei.xml\n",
    "brehm_thierleben_1869.tcf.xml\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lösung A: dtacrawl.py\n",
    "\n",
    "#### Libraries und URL-Hilfsfunktionen (für TEI- sowie TCF-Dateien und JSON-API-Queries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.deutschestextarchiv.de/book/download_xml/brehm_thierleben05_1869\n",
      "https://www.deutschestextarchiv.de/book/download_fulltcf/12345\n",
      "https://kaskade.dwds.de/dstar/dta/dstar.perl?q=abc\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random \n",
    "import time \n",
    "import logging\n",
    "import argparse\n",
    "from urllib.parse import urlunsplit\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\", level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "tei_baseurl = 'https://www.deutschestextarchiv.de/book/download_xml'\n",
    "query_baseurl = 'https://kaskade.dwds.de/dstar/dta/dstar.perl'\n",
    "tcf_baseurl = 'https://www.deutschestextarchiv.de/book/download_fulltcf'\n",
    "\n",
    "def tei_url(basename):\n",
    "    return tei_baseurl + \"/\" + basename\n",
    "\n",
    "def tcf_url(id):\n",
    "    return tcf_baseurl + \"/\" + id\n",
    "\n",
    "def query_url(query):\n",
    "    parts = list(urlsplit(query_baseurl))\n",
    "    parts[3] = urlencode(query)\n",
    "    return urlunsplit(parts)\n",
    "\n",
    "print(tei_url('brehm_thierleben05_1869'))\n",
    "print(tcf_url('12345'))\n",
    "print(query_url({'q': 'abc'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download-Hilsfunktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url):\n",
    "    secs = random.uniform(0.5, 1.5)\n",
    "    time.sleep(secs)\n",
    "    logging.info(f\"downloading {url} after waiting {secs}s\")\n",
    "    with urlopen(url) as f:\n",
    "        return f.read() \n",
    "\n",
    "def download_to(url, out):\n",
    "    with open(out, 'wb') as f:\n",
    "        f.write(download(url))\n",
    "        \n",
    "def query(q):\n",
    "    return json.loads(download(query_url(q)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funktion für Retrieval der DTA-IDs der Suchtreffer eines Korpus-API-Query:\n",
    "\n",
    "- `start`-Parameter = Start der Pagination\n",
    "- bei `limit=3` > Seite 2 startet mit 4 (siehe zweite URL im Output)\n",
    "    - vgl. HTML-Ansicht unter https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=3&start=4\n",
    "- als Default (ohne Angabe) wird Limit auf 10 gesetzt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:04:47: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=3&start=1&fmt=json after waiting 0.6934956890238664s\n",
      "16:04:51: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=3&start=4&fmt=json after waiting 1.0205144961539503s\n",
      "{'25164': 'haeckel_schoepfungsgeschichte_1868', '16241': 'weismann_keimplasma_1892', '25165': 'brehm_thierleben05_1869'}\n"
     ]
    }
   ],
   "source": [
    "def dtaids(max, q):\n",
    "    ids = {}\n",
    "    start = 1\n",
    "    while len(ids) < max:\n",
    "        q[\"limit\"] = max \n",
    "        q[\"start\"] = start\n",
    "        q[\"fmt\"] = \"json\"\n",
    "        hits = query(q)\n",
    "        for hit in hits[\"hits_\"]:\n",
    "            id = hit[\"meta_\"][\"dtaid\"]\n",
    "            if id not in ids:\n",
    "                ids[id] = hit[\"meta_\"][\"basename\"]\n",
    "            if len(ids) == max:\n",
    "                return ids \n",
    "        start = max + start\n",
    "    return ids\n",
    "\n",
    "print(dtaids(3, {'q': 'Axolotl'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download der Korpus-Dateien (TEI und TCF) für die Dokument-Treffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:04:54: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=3&start=1&fmt=json after waiting 0.8054506189337978s\n",
      "16:04:58: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=3&start=4&fmt=json after waiting 1.4683739383820988s\n",
      "16:05:04: downloading https://www.deutschestextarchiv.de/book/download_xml/haeckel_schoepfungsgeschichte_1868 after waiting 0.8962228530395613s\n",
      "16:05:09: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/25164 after waiting 1.274685395392647s\n",
      "16:05:30: downloading https://www.deutschestextarchiv.de/book/download_xml/weismann_keimplasma_1892 after waiting 0.5403405918591221s\n",
      "16:05:34: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/16241 after waiting 1.0757443464914163s\n",
      "16:05:52: downloading https://www.deutschestextarchiv.de/book/download_xml/brehm_thierleben05_1869 after waiting 1.1805839690431699s\n",
      "16:05:59: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/25165 after waiting 1.3063106632810282s\n",
      "16:06:46: done\n"
     ]
    }
   ],
   "source": [
    "    ids = dtaids(3, {'q': 'Axolotl'})\n",
    "    for id in ids:\n",
    "        download_to(tei_url(ids[id]), f\"out/{id}.tei.xml\")\n",
    "        download_to(tcf_url(id), f\"out/{id}.tcf.xml\")\n",
    "    logging.info('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lösung B: dtacrawl_async.py\n",
    "\n",
    "### Asynchroner Crawler\n",
    "* lineares Vorgehen:\n",
    "  1. lade alle Suchergebnisse herunter\n",
    "  2. lade die zugeh&ouml;rigen Dateien herunter\n",
    "* asynchrones Vorgehen:\n",
    "  * lade die Suchergebnisse herunter\n",
    "  * __*parallel dazu:* lade die Dateien herunter sobald Suchergebnisse vorhanden sind__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n",
    "* asynchrone Pfade durch den Code\n",
    "* Threads laufen gleichzeitig auf verschiedenen CPU's (Kernen)\n",
    "* Reihenfolge der Threads ist nicht deterministisch\n",
    "* Kommunikation zwischen den Threads muss synchronisiert werden (Mutex, atomare Variablen ...)\n",
    "* unsynchronisierter Zugriff auf gemeinsame Daten f&uuml;hrt zu Problemen\n",
    "\n",
    "\n",
    "* Python-Library für Threads: `concurrent.futures`\n",
    "\n",
    "#### Einführungsbeispiel:\n",
    "\n",
    "- `thread` als Funktion, die ihre Parameter loggt (mit Zeitverzögerung zum Testen der Parallelisierung)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06:47: Thread 1 producing 0\n",
      "16:06:47: Thread 2 producing 0\n",
      "16:06:48: Thread 2 producing 1\n",
      "16:06:48: Thread 1 producing 1\n",
      "16:06:49: Thread 1 producing 2\n",
      "16:06:49: Thread 2 producing 2\n",
      "16:06:50: Thread 3 producing 0\n",
      "16:06:51: Thread 2 producing 3\n",
      "16:06:51: Thread 3 producing 1\n",
      "16:06:52: Thread 2 producing 4\n",
      "16:06:52: done\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def thread(name, max):\n",
    "    for i in range(max):\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        logging.info(f'{name} producing {i}')\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.submit(thread, \"Thread 1\", 3)\n",
    "    executor.submit(thread, \"Thread 2\", 5)\n",
    "    executor.submit(thread, \"Thread 3\", 2)\n",
    "logging.info('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wettlaufsituation (Race Condition)\n",
    "\n",
    "- Code greift gleichzeitig bzw. nicht in intendierter Reihenfolge auf Variablen zu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06:53: deposit: 37\n",
      "16:06:53: deposit: 20\n",
      "16:06:54: deposit: 3\n",
      "16:06:54: deposit: -10\n",
      "16:06:54: final deposit: -10\n"
     ]
    }
   ],
   "source": [
    "deposit = 50\n",
    "    \n",
    "def withdraw(amount):\n",
    "    global deposit\n",
    "    while True:\n",
    "        if deposit >= amount:\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "            deposit = deposit - amount\n",
    "            logging.info(f'deposit: {deposit}')\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.submit(withdraw, 13)\n",
    "    executor.submit(withdraw, 17)\n",
    "logging.info(f'final deposit: {deposit}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronisation mit Locks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06:55: deposit: 37\n",
      "16:06:57: deposit: 24\n",
      "16:06:58: deposit: 11\n",
      "16:06:58: final deposit: 11\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "locked_deposit = 50\n",
    "lock = threading.Lock()\n",
    "    \n",
    "def withdraw(amount):\n",
    "    global locked_deposit\n",
    "    global lock\n",
    "    while True:       \n",
    "        lock.acquire()\n",
    "        if locked_deposit >= amount:\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "            locked_deposit = locked_deposit - amount\n",
    "            logging.info(f'deposit: {locked_deposit}')\n",
    "            lock.release()\n",
    "        else:\n",
    "            lock.release()\n",
    "            return\n",
    "        \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.submit(withdraw, 13)\n",
    "    executor.submit(withdraw, 17)\n",
    "logging.info(f'final deposit: {locked_deposit}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locks mit `with ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:06:59: deposit: 37\n",
      "16:07:00: deposit: 24\n",
      "16:07:01: deposit: 11\n",
      "16:07:01: final deposit: 11\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "locked_deposit = 50\n",
    "lock = threading.Lock()\n",
    "    \n",
    "def withdraw(amount):\n",
    "    global locked_deposit\n",
    "    global lock\n",
    "    while True:\n",
    "        with lock:    \n",
    "            if locked_deposit >= amount:\n",
    "                time.sleep(random.uniform(0.5, 1.5))\n",
    "                locked_deposit = locked_deposit - amount\n",
    "                logging.info(f'deposit: {locked_deposit}')\n",
    "            else:\n",
    "                return\n",
    "        \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    executor.submit(withdraw, 13)\n",
    "    executor.submit(withdraw, 17)\n",
    "logging.info(f'final deposit: {locked_deposit}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erzeuger-Verbraucher (Consumer Producer) Threading\n",
    "* Erzeuger produzieren Daten\n",
    "* Verbraucher verarbeiten die Daten weiter\n",
    "* Verbraucher und Erzeuger laufen in verschiedenen Threads\n",
    "* Kommunikation (und Synchronisation) der Erzeuger und Verbraucher &uuml;ber Queues\n",
    "* je nach Anwendung verschiedene Anzahlen von Erzeugern und Verbrauchern\n",
    "\n",
    "### Queue\n",
    "* FiFo (First in, first out) Datenstruktur\n",
    "* Elemente werden in der Reihenfolge heraus genommen in der sie eingef&uuml;gt werden\n",
    "\n",
    "![Queue](https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Data_Queue.svg/300px-Data_Queue.svg.png)\n",
    "\n",
    "Quelle: https://en.wikipedia.org/wiki/Queue_(abstract_data_type)#/media/File:Data_Queue.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "* verwendet Pyton Queue implementierung als Basis\n",
    "* dient der Kommunikation zwischen Erzeuger und Verbraucher\n",
    "* Schließen der Pipline signalisiert Ende der Arbeit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "class Pipeline(queue.Queue):\n",
    "    def __init__(self):\n",
    "        super().__init__(maxsize=10)\n",
    "        \n",
    "    def close(self):\n",
    "        self.put((None, None)) # insert sentry\n",
    "        logging.info('pipeline closed')\n",
    "        \n",
    "    def add_url(self, url, out):\n",
    "        self.put((url, out))\n",
    "\n",
    "    def get_url(self):\n",
    "        ret = self.get()\n",
    "        if ret == (None, None):\n",
    "            self.put(ret) # reinsert sentry\n",
    "            return (None, None, False)\n",
    "        else:\n",
    "            return (ret[0], ret[1], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbraucher\n",
    "* ließt URLs aus der Pipline\n",
    "* lädt die entsprechenden Dateien herunter\n",
    "* mehrere parallele Verbraucher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(pipeline):\n",
    "    while True:\n",
    "        url, out, ok = pipeline.get_url()\n",
    "        if not ok:\n",
    "            return \n",
    "        download_to(url, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erzeuger (Producer)\n",
    "* stellt Suchanfragen\n",
    "* schreibt die URLs (tcf und tei) in die Pipeline\n",
    "* signalisiert Ende der Arbeit an die Verbraucher durch Schließen der Pipeline\n",
    "* nur ein Erzeuger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def producer(pipeline, out, max, q):\n",
    "    ids = set()\n",
    "    start = 1\n",
    "    while len(ids) < max:\n",
    "        q[\"limit\"] = max \n",
    "        q[\"start\"] = start \n",
    "        q[\"fmt\"] = \"json\"\n",
    "        hits = query(q)\n",
    "        for hit in hits[\"hits_\"]:\n",
    "            id = hit[\"meta_\"][\"dtaid\"]\n",
    "            basename = hit[\"meta_\"][\"basename\"]\n",
    "            if id not in ids:\n",
    "                ids.add(id)\n",
    "                pipeline.add_url(tei_url(basename), os.path.join(out, f'{id}.tei.xml'))\n",
    "                pipeline.add_url(tcf_url(id), os.path.join(out, f'{id}.tcf.xml'))\n",
    "            if len(ids) == max:\n",
    "                pipeline.close()\n",
    "                return \n",
    "        start = max + start\n",
    "    pipeline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchroner Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:07:02: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=3&start=1&fmt=json after waiting 1.2653087105882421s\n",
      "16:07:06: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=3&start=4&fmt=json after waiting 1.0527904047534369s\n",
      "16:07:07: downloading https://www.deutschestextarchiv.de/book/download_xml/haeckel_schoepfungsgeschichte_1868 after waiting 1.3053905381316873s\n",
      "16:07:07: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/25164 after waiting 1.439911512642786s\n",
      "16:07:09: pipeline closed\n",
      "16:07:10: downloading https://www.deutschestextarchiv.de/book/download_xml/weismann_keimplasma_1892 after waiting 0.8100858010322504s\n",
      "16:07:11: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/16241 after waiting 0.599968221850295s\n",
      "16:07:15: downloading https://www.deutschestextarchiv.de/book/download_xml/brehm_thierleben05_1869 after waiting 1.0681195739987117s\n",
      "16:07:28: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/25165 after waiting 0.7942733285939677s\n",
      "16:08:29: done\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.submit(producer, pipeline, 'out', 3, {'q': 'Axolotl'})\n",
    "    executor.submit(consumer, pipeline)\n",
    "    executor.submit(consumer, pipeline)\n",
    "    executor.submit(consumer, pipeline)\n",
    "logging.info('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline als Kontextmanagerobjekt\n",
    "* die `with ... as ...` Syntax erm&ouml;glicht das automatische Schließen von Resourcen (Dateihandel...)\n",
    "* mit `contextmanager` Objekten k&ouml;nnen eigene Klassen mit `with ... as ...` verwendet werden\n",
    "* `contextmanager` Objekte in Python m&uuml;ssen zwei Metoden implementieren\n",
    "  1. `__enter__` gibt das mit `as` referenzierte Objekt zur&uuml;ck (wird automatisch geschlossen)\n",
    "  2. `__exit__` steuert die Fehlerbehandlung\n",
    "* genaueres in der Python [Dokumentation](https://docs.python.org/3/library/stdtypes.html#typecontextmanager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:08:30: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=2&start=1&fmt=json after waiting 0.6286216931088301s\n",
      "16:08:34: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=2&start=3&fmt=json after waiting 0.9097789644972452s\n",
      "16:08:34: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/25164 after waiting 1.3418201584607246s\n",
      "16:08:34: downloading https://www.deutschestextarchiv.de/book/download_xml/haeckel_schoepfungsgeschichte_1868 after waiting 1.3778800391157429s\n",
      "16:08:37: pipeline closed\n",
      "16:08:38: downloading https://www.deutschestextarchiv.de/book/download_xml/weismann_keimplasma_1892 after waiting 1.2849920341912444s\n",
      "16:08:40: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/16241 after waiting 1.091954962302415s\n",
      "16:09:10: done\n"
     ]
    }
   ],
   "source": [
    "class PipelineCM(Pipeline):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __enter__(self):\n",
    "        return self \n",
    "    def __exit__(self, et, ev, etb):\n",
    "        self.close()\n",
    "        return False\n",
    "\n",
    "def producer2(pipeline, out, max, q):\n",
    "    ids = set()\n",
    "    start = 1\n",
    "    with pipeline as p:\n",
    "        while len(ids) < max:\n",
    "            q[\"limit\"] = max \n",
    "            q[\"start\"] = start \n",
    "            q[\"fmt\"] = \"json\"\n",
    "            hits = query(q)\n",
    "            for hit in hits[\"hits_\"]:\n",
    "                id = hit[\"meta_\"][\"dtaid\"]\n",
    "                basename = hit[\"meta_\"][\"basename\"]\n",
    "                if id not in ids:\n",
    "                    ids.add(id)\n",
    "                    p.add_url(tei_url(basename), os.path.join(out, f'{id}.tei.xml'))\n",
    "                    p.add_url(tcf_url(id), os.path.join(out, f'{id}.tcf.xml'))\n",
    "                if len(ids) == max:\n",
    "                    return \n",
    "            start = max + start \n",
    "\n",
    "pipeline = PipelineCM()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.submit(producer2, pipeline, 'out', 2, {'q': 'Axolotl'})\n",
    "    executor.submit(consumer, pipeline)\n",
    "    executor.submit(consumer, pipeline)\n",
    "    executor.submit(consumer, pipeline)\n",
    "logging.info('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time\n",
    "\n",
    "| Name | time - misst die Laufzeit von Programmen |\n",
    "|:---|:---|\n",
    "|Überblick| time \\[OPTION\\]... \\[CMD\\] \\[ARGS\\]... |\n",
    "|Beschreibung | Misst die Laufzeit von Programmen |\n",
    "| Wichtige Optionen: | |\n",
    "| -v, --verbose | detailierte Ausgabe |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:09:11: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=2&start=1&fmt=json after waiting 0.9914523984143253s\n",
      "16:09:15: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=2&start=3&fmt=json after waiting 0.5640277921646498s\n",
      "16:09:18: downloading https://www.deutschestextarchiv.de/book/download_xml/haeckel_schoepfungsgeschichte_1868 after waiting 0.5693842343056343s\n",
      "16:09:22: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/25164 after waiting 0.890126520972706s\n",
      "16:09:43: downloading https://www.deutschestextarchiv.de/book/download_xml/weismann_keimplasma_1892 after waiting 0.6919032067725522s\n",
      "16:09:47: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/16241 after waiting 0.9144674417724756s\n",
      "\n",
      "real\t0m52.728s\n",
      "user\t0m1.521s\n",
      "sys\t0m1.610s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "time python3 dtacrawl.py --max 2 --dir out 'Axolotl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:10:05: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=2&start=1&fmt=json after waiting 0s\n",
      "16:10:08: downloading https://kaskade.dwds.de/dstar/dta/dstar.perl?q=Axolotl&limit=2&start=3&fmt=json after waiting 0s\n",
      "16:10:08: downloading https://www.deutschestextarchiv.de/book/download_xml/haeckel_schoepfungsgeschichte_1868 after waiting 0s\n",
      "16:10:08: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/25164 after waiting 0s\n",
      "16:10:11: pipeline closed\n",
      "16:10:12: downloading https://www.deutschestextarchiv.de/book/download_xml/weismann_keimplasma_1892 after waiting 0s\n",
      "16:10:13: downloading https://www.deutschestextarchiv.de/book/download_fulltcf/16241 after waiting 0s\n",
      "\n",
      "real\t0m42.906s\n",
      "user\t0m1.615s\n",
      "sys\t0m1.378s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "time python3 dtacrawl_async.py --max 2 --dir out 'Axolotl'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "656953c21547a91cfb2057a5cd28b058012b7084abb49e89974147a4ece772c4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

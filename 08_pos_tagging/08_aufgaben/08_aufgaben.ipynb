{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgaben 8\n",
    "\n",
    "## Aufgabe 1 (Vergleich von POS-Taggern)\n",
    "\n",
    "Laden Sie vier weitere \n",
    "[TCF-kodierte](https://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/The_TCF_Format)\n",
    "Dateien herunter\n",
    "(z.B. [Drude](https://www.deutschestextarchiv.de/book/download_fulltcf/16377),\n",
    "[Sachs](https://www.deutschestextarchiv.de/book/download_fulltcf/16178),\n",
    "[Brehm](https://www.deutschestextarchiv.de/book/download_fulltcf/25157)\n",
    "und\n",
    "[Bölsche](https://www.deutschestextarchiv.de/book/download_fulltcf/16552)).\n",
    "Verwenden Sie diese Dateien und die von letzter Woche\n",
    "([Altmann](https://www.deutschestextarchiv.de/book/download_fulltcf/16299))\n",
    "um wie heute in der Vorlesung einen Bigramm-Tagger auf einer\n",
    "Trainingsmenge zu trainieren und auf einer Testmenge auszuwerten.\n",
    "\n",
    "Installieren Sie den TreeTagger und den Treetaggerwrapper und\n",
    "schreiben Sie eine entsprechende Tagger-Klasse, die Token mit Hilfe\n",
    "des TreeTaggers annotiert.  Werten Sie diesen Tagger auf der\n",
    "Trainingsmenge aus und vergleichen Sie die Ergebnisse mit denen des\n",
    "Bigramm-Taggers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install --upgrade pip nltk\n",
    "#!{sys.executable} -m pip install --upgrade pip treetaggerwrapper\n",
    "import nltk\n",
    "import treetaggerwrapper as TTW \n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "import logging \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "ns = {'tc': \"http://www.dspin.de/data/textcorpus\"}\n",
    "urls = [\n",
    "    'https://www.deutschestextarchiv.de/book/download_fulltcf/16377',\n",
    "    'https://www.deutschestextarchiv.de/book/download_fulltcf/16178',\n",
    "    'https://www.deutschestextarchiv.de/book/download_fulltcf/25157',\n",
    "    'https://www.deutschestextarchiv.de/book/download_fulltcf/16552',\n",
    "    'https://www.deutschestextarchiv.de/book/download_fulltcf/16299',\n",
    "]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_from_tcf(root):\n",
    "    token_ids = {t.attrib[\"ID\"]: t.text for t in root.find('tc:TextCorpus', ns).find('tc:tokens', ns)}\n",
    "    pos_tags = {t.attrib[\"tokenIDs\"]: t.text for t in root.find('tc:TextCorpus', ns).find('tc:POStags', ns)}\n",
    "    return [[(token_ids[id], pos_tags[id]) for id in sent.attrib['tokenIDs'].split(\" \")] for sent in root.find('tc:TextCorpus', ns).find('tc:sentences', ns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_xml_from_url(url):\n",
    "    # Get the file name of the url.\n",
    "    path = urllib.parse.urlparse(url)\n",
    "    _, filename = os.path.split(path.path)\n",
    "    filename += \".xml\"\n",
    "    # Check if sentence file is cached.\n",
    "    if os.path.isfile(filename):\n",
    "        logging.info(f\"reading from cache: {filename}\")\n",
    "        with open(filename) as f:\n",
    "            return f.read()\n",
    "    # Download file.\n",
    "    logging.debug(f\"downloading from url: {url}\")\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        contents = f.read()    \n",
    "    logging.info(f\"caching to file: {filename}\")    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(contents)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def read_sentences_from_url(url):\n",
    "    # Get the file name of the url.\n",
    "    path = urllib.parse.urlparse(url)\n",
    "    _, filename = os.path.split(path.path)\n",
    "    filename += \".sents.txt\"\n",
    "    # Check if sentence file is cached.\n",
    "    if os.path.isfile(filename):\n",
    "        logging.info(f\"reading from cache: {filename}\")\n",
    "        with open(filename) as f:\n",
    "            # Read sentences from the cached file.\n",
    "            return [[nltk.str2tuple(t) for t in sent[:-1].split(\" \")] for sent in f.readlines()]\n",
    "\n",
    "    # File is not cached; download it and cache it.\n",
    "    sents = tags_from_tcf(ET.fromstring(read_xml_from_url(url)))\n",
    "    logging.info(f\"caching to file: {filename}\")    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for sent in sents:\n",
    "            f.write(\" \".join([t[0] + \"/\" + t[1] for t in sent]))\n",
    "            f.write(\"\\n\")\n",
    "    # Return sentences.\n",
    "    return sents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reading from cache: 16377.sents.txt\n",
      "INFO:root:reading from cache: 16178.sents.txt\n",
      "INFO:root:reading from cache: 25157.sents.txt\n",
      "INFO:root:reading from cache: 16552.sents.txt\n",
      "INFO:root:reading from cache: 16299.sents.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 42269\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = [sent for url in urls for sent in read_sentences_from_url(url)]\n",
    "print(\"number of sentences:\", len(tagged_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramm-Tagger mit Unigramm-Tagger als Backoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set:  4227\n",
      "train set: 38042\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(13)\n",
    "random.shuffle(tagged_sents)\n",
    "size = int(len(tagged_sents) * 0.9)\n",
    "test_sents = tagged_sents[size:]\n",
    "train_sents = tagged_sents[:size]\n",
    "print(\"test set: \", len(test_sents))\n",
    "print(\"train set:\", len(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training und Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9381128823782853"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Beispiel für Implementierung von eigenem POS-Tagger mit NLTK (mit SequentialBackoffTagger):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  ['Hierzu', 'noch', 'einige', 'Erläuterungen', '.']\n",
      "index:   0\n",
      "history: []\n",
      "tokens:  ['Hierzu', 'noch', 'einige', 'Erläuterungen', '.']\n",
      "index:   1\n",
      "history: ['NN']\n",
      "tokens:  ['Hierzu', 'noch', 'einige', 'Erläuterungen', '.']\n",
      "index:   2\n",
      "history: ['NN', 'NN']\n",
      "tokens:  ['Hierzu', 'noch', 'einige', 'Erläuterungen', '.']\n",
      "index:   3\n",
      "history: ['NN', 'NN', 'NN']\n",
      "tokens:  ['Hierzu', 'noch', 'einige', 'Erläuterungen', '.']\n",
      "index:   4\n",
      "history: ['NN', 'NN', 'NN', 'NN']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogTagger(nltk.tag.sequential.SequentialBackoffTagger):\n",
    "    def __init__(self, backoff=None):\n",
    "        super().__init__(backoff)\n",
    "\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        print(\"tokens: \", tokens)\n",
    "        print(\"index:  \", index)\n",
    "        print(\"history:\", history)\n",
    "        return None \n",
    "\n",
    "for sent in test_sents:\n",
    "    if len(sent) == 5:\n",
    "        short_sent = sent \n",
    "\n",
    "tx = LogTagger(backoff=t0)\n",
    "tx.evaluate([short_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich mit TreeTagger:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementierung TreeTagger als NLTK-POS-Tagger (erbt von SequentialBackoffTagger) über TTW (Treetaggerwrapper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zur Umwandlung von TreeTagger-Tripel zu Tupel:\n",
    "def tripple2tupple(str):\n",
    "    tmp = str.split(\"\\t\")\n",
    "    return (tmp[0], tmp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7903317535545024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.disable(logging.DEBUG)\n",
    "logging.disable(logging.INFO)\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "class TreeTagger(nltk.tag.sequential.SequentialBackoffTagger):\n",
    "    def __init__(self, language, directory, backoff=None):\n",
    "        super().__init__(backoff)\n",
    "        self.tagger = TTW.TreeTagger(TAGLANG=language, TAGDIR=directory, TAGOPT='-token -sgml -quiet')\n",
    "        self.tags = None\n",
    "\n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        if index == 0: #neuer Satz\n",
    "            self.tags = [tripple2tupple(tripple) for tripple in self.tagger.tag_text(\" \".join(tokens))]\n",
    "        if index < len(self.tags):\n",
    "            return self.tags[index][1]\n",
    "        return None\n",
    "\n",
    "### ohne TAGOPT-Angaben: Fehler (TTW): Time out for TreeTagger (wegen Fehler in TreeTagger: hält Prozess an)\n",
    "##Debugging: ohne lemma-Option in TAGOPT (TreeTagger-Lemmatisierung produziert Fehler bei bestimmten Tokens)\n",
    "\n",
    "tt = TreeTagger('de', '../tree-tagger', t0)\n",
    "tt.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7903317535545024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = TreeTagger('de', '../tree-tagger', t2)\n",
    "tt.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316415338216286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = TreeTagger('de', '../tree-tagger', t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=tt)\n",
    "t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich mit domänenfremden Testsets\n",
    "\n",
    "- Treetagger: allgemeines deutsches Modell \n",
    "- hier trainierter Bigramm-Tagger: trainiert auf Korpus mit historischen zoologischen Büchern (deshalb besseres Ergebnis auf den entsprechenden Testdaten)\n",
    "\n",
    "- jetzt: andere Testsets (domänenfremde Texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8864010737599902"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://www.deutschestextarchiv.de/book/download_fulltcf/34066'\n",
    "tagged_sents = read_sentences_from_url(url)\n",
    "\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "t2.evaluate(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8629125739735221"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = TreeTagger('de', '../tree-tagger', t0)\n",
    "tt.evaluate(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9168110918544194"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.deutschestextarchiv.de/book/download_fulltcf/32274'\n",
    "tagged_sents = read_sentences_from_url(url)\n",
    "t2.evaluate(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9410745233968805"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.evaluate(tagged_sents)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "656953c21547a91cfb2057a5cd28b058012b7084abb49e89974147a4ece772c4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgaben 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1: xmlpos (Extraktion und Annotation von TEI-XML)\n",
    "Schreiben Sie ein Python-Programm `xmlpos`, das aus einer Korpusdatei\n",
    "den Dokumenteninhalt extrahiert, diesen dann satzweise POS-tagged\n",
    "und eine XML-Datei folgender Form ausgibt:\n",
    "\n",
    "```xml\n",
    "<doc>\n",
    "  <s id=\"s-0001\">\n",
    "    <w id=\"w-0001\" pos=\"DET\">Der</w>\n",
    "\t<w id=\"w-0002\" pos=\"NOUN\">Hase</w>\n",
    "\t...\n",
    "  </s>\n",
    "  <s id=\"s-0002\">\n",
    "  ...\n",
    "</doc>\n",
    "```\n",
    "\n",
    "Anmerkungen:\n",
    "* Verwendenden Sie die [TEI](https://tei-c.org/)-kodierten Korpusdateien des [deutschen Textarchivs](https://www.deutschestextarchiv.de/), z.B:\n",
    "  * Altman 1890: https://www.deutschestextarchiv.de/book/download_xml/altmann_elementarorganismen_1890\n",
    "  * Brandes 1832: https://www.deutschestextarchiv.de/book/download_xml/brandes_naturlehre03_1832\n",
    "\n",
    "\n",
    "> _XML-TEI (Text Encoding Initiative): weit verbreitetes XML-Dokumentenformat für die Kodierung von Textkorpora_    \n",
    "  (Schema mit einheitlichen Elementnamen und Attributen für die editionswissenschaftliche und linguistische Annotation von Texten, u.a. über DTD definiert)\n",
    "\n",
    "> _DTA (Deutsches Textarchiv): historisches deutsches Korpus_ (linguistisch annotiertes Volltextkorpus deutschsprachiger Texte aus der Zeit um 1600 bis 1900)\n",
    "      \n",
    "\n",
    "* Sie können die XML-Dateien auch mit Werkzeugen wie wget oder curl\n",
    "  (man-Pages!)  herunterladen, oder auch Pythons `urllib` verwenden\n",
    "* Sie können `nltk` zur Satzsegmentierung und zum POS-Tagging verwenden; alternativ\n",
    "  dazu können Sie auch _stanza_ (oder _[spacy](https://spacy.io/)_) verwenden\n",
    "* Stellen Sie sicher, dass der tatsächlichen Text aus den Dokumenten extrahiert wird;\n",
    "  betrachten Sie hierzu folgendes (valides) XML-Dokument: `<a><b>1<c>2<d/>3</c></b>4<lb/>5</a>` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [F-Strings](http://cissandbox.bentley.edu/sandbox/wp-content/uploads/2022-02-10-Documentation-on-f-strings-Updated.pdf)\n",
    "* [Spacy](https://spacy.io/)\n",
    "* [DTA](https://www.deutschestextarchiv.de/)\n",
    "* [TEI](https://github.com/TEIC/TEI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehen:\n",
    "\n",
    "1. XML-Dokument herunterladen (mit `requests`-Library oder `urllib`-Library)\n",
    "2. Text (Inhalt von `<text>`-TEI-Element) mit `etree` aus Datei extrahieren:\n",
    "   - mit XML-Parser über das XML-File iterieren (Namespace des TEI-Wurzelelements berücksichtigen!)\n",
    "   -  Verwendung von Text-Extractor-Klasse (`TreeExtractor`; alternativ mit Funktionsaufruf: `gather_text`) und Hilfsfunktionen (`append_text`)\n",
    "3. Satzsegmentierung(mit `NLTK.sent_tokenize()` und POS-Tagging (hier mit `spacy`)\n",
    "4. Aufbau der neuen XML-Struktur mit `etree` (Satz-Segment- und Wort-POS-Informationen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEI-XML-Grundstruktur (Altmann-BSP):\n",
    "```xml\n",
    "<TEI xmlns=\"http://www.tei-c.org/ns/1.0\">\n",
    "    <teiHeader>\n",
    "        ...\n",
    "    </teiHeader>\n",
    "    <text>\n",
    "        <front>\n",
    "            <titlePage type=\"halftitle\">\n",
    "                ...\n",
    "            </titlePage>\n",
    "        </front>\n",
    "        <body>\n",
    "            <div n=\"1\">\n",
    "                ...\n",
    "                <p>\n",
    "                    Seitdem von\n",
    "                    <hi rendition=\"#k\">Dujardin</hi>\n",
    "                    die contraktile Substanz oder Sar¬\n",
    "                    <lb/>\n",
    "                    kode entdeckt war, hat dieselbe in Bezug auf die Deutung ihres\n",
    "                    <lb/>\n",
    "                    ...\n",
    "                </p>\n",
    "            </div>\n",
    "        </body>\n",
    "        <back>\n",
    "            ...\n",
    "        </back>\n",
    "    </text>\n",
    "</TEI>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append_text\n",
    "\n",
    "- setzt getrennte Wörter am Zeilenende (`¬</lb>`) zusammen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<abc def>>\n",
      "<<ungelegen>>\n"
     ]
    }
   ],
   "source": [
    "def append_text(text, str):\n",
    "    if not str or str == \"\":\n",
    "        return text \n",
    "    \n",
    "    str = str.strip()\n",
    "    if text == \"\":\n",
    "        return str \n",
    "    if text[-1] == '¬': # Trennerzeichen in XML-File\n",
    "        return text[:-1] + str \n",
    "    return text + \" \" + str \n",
    "\n",
    "print(f'<<{append_text(\"abc\", \"   def \")}>>')\n",
    "print(f'<<{append_text(\"unge¬\", \"legen\")}>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreeExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<1 2>>\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "# Achtung: um auch den Text vor dem schließenden Element zu erhalten: tail-Attribut verwenden\n",
    "\n",
    "# hier zunächst ohne tail:\n",
    "\n",
    "class TextExtractor:\n",
    "    def __init__(self, root):\n",
    "        self.root = root \n",
    "        \n",
    "    def extract(self):\n",
    "        self.text = \"\"\n",
    "        self.extract_rec(self.root) #Iterieren über Kinderknoten mit rekursiver Funktion\n",
    "        return self.text\n",
    "        \n",
    "    def extract_rec(self, node):\n",
    "        self.text = append_text(self.text, node.text)\n",
    "        for c in node:  #Iterieren über Kinderknoten\n",
    "            self.extract_rec(c) #rekursiver Aufruf (für Extraktion des Texts aus Kinder-Knoten)\n",
    "        #self.text = append_text(self.text, node.tail) #tail für Elemente vor dem schließenden Tag (im BSP: 4 und 5)\n",
    "        \n",
    "e = TextExtractor(ET.fromstring('<a><b>1<c>2<d/>3</c></b>4<lb/>5</a>'))\n",
    "print(f'<<{e.extract()}>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<1 2 3 4 5>>\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "class TextExtractor:\n",
    "    def __init__(self, root):\n",
    "        self.root = root \n",
    "        \n",
    "    def extract(self):\n",
    "        self.text = \"\"\n",
    "        self.extract_rec(self.root) #Iterieren über Kinderknoten mit rekursiver Funktion\n",
    "        return self.text\n",
    "        \n",
    "    def extract_rec(self, node):\n",
    "        self.text = append_text(self.text, node.text)\n",
    "        for c in node:  #Iterieren über Kinderknoten\n",
    "            self.extract_rec(c) #rekursiver Aufruf (für Extraktion des Texts aus Kinder-Knoten)\n",
    "        self.text = append_text(self.text, node.tail) #tail für Elemente vor dem schließenden Tag (im BSP: 4 und 5)\n",
    "        \n",
    "e = TextExtractor(ET.fromstring('<a><b>1<c>2<d/>3</c></b>4<lb/>5</a>'))\n",
    "print(f'<<{e.extract()}>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1: gather_text (Verwendung siehe xmlpos.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<1 2 3 4 5>>\n"
     ]
    }
   ],
   "source": [
    "def gather_text(node, text):\n",
    "    text = append_text(text, node.text)\n",
    "    for c in node: #Iterieren über Kinder-Knoten\n",
    "        text = gather_text(c, text) #rekursiver Aufruf\n",
    "    #return text\n",
    "    return append_text(text, node.tail)\n",
    "\n",
    "root = ET.fromstring('<a><b>1<c>2<d/>3</c></b>4<lb/>5</a>')\n",
    "text = gather_text(root, \"\")\n",
    "print(f'<<{text}>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2: gather_text_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<1 2 3 4 5>>\n"
     ]
    }
   ],
   "source": [
    "def gather_text_alt(node):\n",
    "    return gather_text_rec(node, \"\")\n",
    "\n",
    "def gather_text_rec(node, text):\n",
    "    text = append_text(text, node.text)\n",
    "    for c in  node:\n",
    "        text = gather_text_rec(c, text)\n",
    "    return append_text(text, node.tail)\n",
    "    \n",
    "root = ET.fromstring('<a><b>1<c>2<d/>3</c></b>4<lb/>5</a>')\n",
    "text = gather_text_alt(root)\n",
    "print(f'<<{text}>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download der XML-Dateien und Verwendung der TextExtractor-Klasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zu sichern. Das Bewusstsein, dass uns hier die Grundprobleme der Biologie berühren, wird es hoffen\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "import urllib.request\n",
    "url = \"https://www.deutschestextarchiv.de/book/download_xml/altmann_elementarorganismen_1890\"\n",
    "# url = \"https://www.deutschestextarchiv.de/book/download_xml/brandes_naturlehre03_1832\"\n",
    "\n",
    "\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    root = ET.fromstring(f.read())\n",
    "\n",
    "# TEI-Namespace (Namespace als Präfix der Elementnamen, z.B. http://www.tei-c.org/ns/1.0:element)\n",
    "ns = {'tei': 'http://www.tei-c.org/ns/1.0'} #Namespace-Dictionary mit Abkürzungen\n",
    "\n",
    "# Extraktion mit <tei:text> als Wurzelknoten (d.h. ohne Metadaten in <teiHeader>):\n",
    "e = TextExtractor(root.find('tei:text', ns))\n",
    "text = e.extract()\n",
    "print(text[1002:1100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download des [Spacy](https://spacy.io) Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install --upgrade pip spacy\n",
    "import spacy\n",
    "#!{sys.executable} -m pip install --upgrade pip nltk\n",
    "from nltk import sent_tokenize\n",
    "import xml.dom.minidom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tagging und Aufbau des XML-Baums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<doc>\n",
      "\t<s id=\"s-00001\">\n",
      "\t\t<w id=\"w-00001\" pos=\"DET\">DIE</w>\n",
      "\t\t<w id=\"w-00002\" pos=\"NOUN\">ELEMENTARORGANISMEN</w>\n",
      "\t\t<w id=\"w-00003\" pos=\"PROPN\">BEZIEHUNGEN</w>\n",
      "\t\t<w id=\"w-00004\" pos=\"ADP\">ZU</w>\n",
      "\t\t<w id=\"w-00005\" pos=\"PROPN\">DEN</w>\n",
      "\t\t<w id=\"w-00006\" pos=\"PROPN\">ZELLEN</w>\n",
      "\t\t<w id=\"w-00007\" pos=\"ADP\">VON</w>\n",
      "\t\t<w id=\"w-00008\" pos=\"SPACE\"> </w>\n",
      "\t\t<w id=\"w-00009\" pos=\"PROPN\">RICHARD</w>\n",
      "\t\t<w id=\"w-00010\" pos=\"PROPN\">ALTMANN</w>\n",
      "\t\t<w id=\"w-00011\" pos=\"PROPN\">MIT</w>\n",
      "\t\t<w id=\"w-00012\" pos=\"PROPN\">ZWEI</w>\n",
      "\t\t<w id=\"w-00013\" pos=\"PROPN\">ABBILDUNGEN</w>\n",
      "\t\t<w id=\"w-00014\" pos=\"ADP\">IM</w>\n",
      "\t\t<w id=\"w-00015\" pos=\"PROPN\">TEXT</w>\n",
      "\t\t<w id=\"w-00016\" pos=\"CCONJ\">UND</w>\n",
      "\t\t<w id=\"w-00017\" pos=\"PROPN\">XXI</w>\n",
      "\t\t<w id=\"w-00018\" pos=\"PROPN\">TAFELN</w>\n",
      "\t\t<w id=\"w-00019\" pos=\"PUNCT\">.</w>\n",
      "\t</s>\n",
      "\t<s id=\"s-00002\">\n",
      "\t\t<w id=\"w-00020\" pos=\"PROPN\">LEIPZIG</w>\n",
      "\t\t<w id=\"w-00021\" pos=\"PUNCT\">,</w>\n",
      "\t\t<w id=\"w-00022\" pos=\"PROPN\">VERLAG</w>\n",
      "\t\t<w id=\"w-00023\" pos=\"PROPN\">VON</w>\n",
      "\t\t<w id=\"w-00024\" pos=\"PROPN\">VEIT</w>\n",
      "\t\t<w id=\"w-00025\" pos=\"CCONJ\">&amp;</w>\n",
      "\t\t<w id=\"w-00026\" pos=\"PROPN\">COMP</w>\n",
      "\t\t<w id=\"w-00027\" pos=\"PUNCT\">.</w>\n",
      "\t</s>\n",
      "\t<s id=\"s-00003\">\n",
      "\t\t<w id=\"w-00028\" pos=\"NUM\">1890.</w>\n",
      "\t</s>\n",
      "\t<s id=\"s-00004\">\n",
      "\t\t<w id=\"w-00029\" pos=\"DET\">DIE</w>\n",
      "\t\t<w id=\"w-00030\" pos=\"NOUN\">ELEMENTARORGANISMEN</w>\n",
      "\t\t<w id=\"w-00031\" pos=\"PROPN\">BEZIEHUNGEN</w>\n",
      "\t\t<w id=\"w-00032\" pos=\"ADP\">ZU</w>\n",
      "\t\t<w id=\"w-00033\" pos=\"PROPN\">DEN</w>\n",
      "\t\t<w id=\"w-00034\" pos=\"PROPN\">ZELLEN</w>\n",
      "\t\t<w id=\"w-00035\" pos=\"PUNCT\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('de_core_news_sm')\n",
    "out = ET.Element('doc') #für Erzeugung XML-File\n",
    "sid = 1 # sentence id\n",
    "wid = 1 # word id\n",
    "\n",
    "#Satzsegmentierung mit NLTK-Methode:\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "#POS-Tagging mit spaCy und Erzeugung XML-File:\n",
    "for sent in sents:\n",
    "    stag = ET.SubElement(out, 's') #Erzeugung von s-Element\n",
    "    stag.attrib = {'id': f's-{sid:05d}'}\n",
    "    sid += 1\n",
    "    tokens = nlp(sent) #Analyse mit spaCy\n",
    "    for token in tokens: #Erzeugen von w-Elementen (pro Satz)\n",
    "        wtag = ET.SubElement(stag, 'w')\n",
    "        wtag.text = token.text\n",
    "        wtag.attrib = {'pos': token.pos_, 'id': f'w-{wid:05d}'}\n",
    "        wid += 1\n",
    "\n",
    "dom = xml.dom.minidom.parseString(ET.tostring(out))\n",
    "print(dom.toprettyxml()[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage von xmlpos.py (mit Ausgabe in Datei):\n",
    "\n",
    "#### Einlesen aus Datei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat altmann_elementarorganismen_1890.TEI-P5.xml | python3 xmlpos.py > altmann_elementarorganismen_1890.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\n",
      "<doc>\n",
      "  <s id=\"s-1\">\n",
      "    <w id=\"w-1\" pos=\"DET\">DIE</w>\n",
      "    <w id=\"w-2\" pos=\"SPACE\"> </w>\n",
      "    <w id=\"w-3\" pos=\"PROPN\">ELEMENTARORGANISMEN</w>\n",
      "    <w id=\"w-4\" pos=\"CCONJ\">UND</w>\n",
      "    <w id=\"w-5\" pos=\"PROPN\">IHRE</w>\n",
      "    <w id=\"w-6\" pos=\"SPACE\"> </w>\n",
      "    <w id=\"w-7\" pos=\"PROPN\">BEZIEHUNGEN</w>\n",
      "    <w id=\"w-8\" pos=\"ADP\">ZU</w>\n",
      "    <w id=\"w-9\" pos=\"PROPN\">DEN</w>\n",
      "    <w id=\"w-10\" pos=\"PROPN\">ZELLEN</w>\n",
      "    <w id=\"w-11\" pos=\"PUNCT\">.</w>\n",
      "  </s>\n",
      "  <s id=\"s-2\">\n",
      "    <w id=\"w-12\" pos=\"ADP\">VON</w>\n",
      "    <w id=\"w-13\" pos=\"SPACE\">  </w>\n",
      "    <w id=\"w-14\" pos=\"PROPN\">RICHARD</w>\n",
      "    <w id=\"w-15\" pos=\"PROPN\">ALTMANN</w>\n",
      "    <w id=\"w-16\" pos=\"PUNCT\">.</w>\n",
      "  </s>\n",
      "  <s id=\"s-3\">\n",
      "    <w id=\"w-17\" pos=\"PROPN\">MIT</w>\n",
      "    <w id=\"w-18\" pos=\"PROPN\">ZWEI</w>\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "xmllint --format altmann_elementarorganismen_1890.xml | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Herunterladen der XML-Datei mit curl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      " 22  385k   22 90112    0     0  49558      0  0:00:07  0:00:01  0:00:06 49539\r",
      "100  385k  100  385k    0     0   202k      0  0:00:01  0:00:01 --:--:--  202k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl https://www.deutschestextarchiv.de/book/download_xml/altmann_elementarorganismen_1890 | python3 xmlpos.py > altmann_elementarorganismen_1890.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Aufgabe 2: xmltcf (Verarbeitung von annotierten XML-Dateien im TCF-Format)\n",
    "\n",
    "Im deutschen Textarchiv findet man auch annotierte Daten zu den\n",
    "entsprechenden Dokumenten.  Laden Sie eine solche lemmatisierte und\n",
    "annotierte XML-Datei im [TCF-Format](https://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/The_TCF_Format) (Text Corpus Format)\n",
    "herunter (z.B. Altmann: https://www.deutschestextarchiv.de/book/download_fulltcf/16299)\n",
    "und schreiben Sie ein Programm `xmltcf`, das eine zeilenweise die Sätze\n",
    "ausgibt, wobei die Token entweder mit ihren Lemmata oder ihren Tags\n",
    "annotiert sind:\n",
    "```bash\n",
    "$ ./xmltcf.py file.xml\n",
    "Ich/ich gehe/gehen ...\n",
    "$ ./xmltcf.py --tags file.xml\n",
    "Der/DET Mann/NOUN ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehen:\n",
    "\n",
    "- Iterieren über Sätze (`<sentences>`) und Tokens (`<tokens>`) des TCF-XML-Files\n",
    "- Lookup von POS (`<POStags>`) und Lemmata (`<lemmas>`) im XML über `tokenIDs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCF-XML-Grundstruktur:\n",
    "```xml\n",
    "<D-Spin xmlns=\"http://www.dspin.de/data\" version=\"0.4\">\n",
    "    <MetaData xmlns=\"http://www.dspin.de/data/metadata\">\n",
    "        ...\n",
    "    </MetaData>\n",
    "    <TextCorpus  xmlns=\"http://www.dspin.de/data/textcorpus\" lang=\"de\">\n",
    "        <tokens>\n",
    "            <token ID=\"w1\">zwei</token>\n",
    "            <token ID=\"w2\">Tests</token>\n",
    "        </tokens>\n",
    "        <sentences>\n",
    "            <sentence ID=\"s1\" tokenIDs=\"w1 w2\"/>\n",
    "        </sentences>\n",
    "        <lemmas>\n",
    "            <lemma tokenIDs=\"w1\">zwei</lemma>\n",
    "            <lemma tokenIDs=\"w2\">Test</lemma>\n",
    "        </lemmas>\n",
    "        <POStags tagset=\"stts\">\n",
    "            <tag tokenIDs=\"w1\">ART</tag>\n",
    "            <tag tokenIDs=\"w2\">NN</tag>\n",
    "        </POStags>\n",
    "        <orthography>\n",
    "            ...\n",
    "        </orthography>\n",
    "    </TextCorpus>\n",
    "</D-Spin>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xmltcf.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "import argparse\n",
    "\n",
    "# Token-Daten-Klasse: speichert Token, ID, POS-Tag und Lemma\n",
    "class Token:\n",
    "    def __init__(self, word, id, tag=None, lemma=None):\n",
    "        self.word = word\n",
    "        self.id = id\n",
    "        self.tag = tag\n",
    "        self.lemma = lemma\n",
    "\n",
    "# url = 'https://www.deutschestextarchiv.de/book/download_fulltcf/16299'\n",
    "\n",
    "def parse_xml(url, tags):\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        root = ET.fromstring(f.read())\n",
    "\n",
    "#    with open(name, encoding='utf-8') as f:\n",
    "#        root = ET.fromstring(f.read())\n",
    "        \n",
    "    #Namespace-Dictionary (TextCorpus-Namespace):\n",
    "    ns = {'tc': \"http://www.dspin.de/data/textcorpus\"} \n",
    "    \n",
    "    #1. Iterieren über Token-Elemente im TextCorpus-Knoten, Generierung lex-Dictionary mit Tokens+IDs:\n",
    "    # (Verwendung von Token-Datenklasse (oben definiert) in Dictionary-Comprehension)\n",
    "    lex = {t.attrib[\"ID\"]: Token(t.text, t.attrib[\"ID\"]) for t in root.find('tc:TextCorpus', ns).find('tc:tokens', ns)}\n",
    "\n",
    "    #2. Iterieren über Lemmas- und POSTags-Elemente und Ergänzung in lex-Dictionary (über tokenIDs):\n",
    "    for lemma in root.find('tc:TextCorpus', ns).find('tc:lemmas', ns ):\n",
    "        token = lex[lemma.attrib[\"tokenIDs\"]]\n",
    "        token.lemma = lemma.text\n",
    "    for tag in root.find('tc:TextCorpus', ns ).find('tc:POStags', ns):\n",
    "        token = lex[tag.attrib[\"tokenIDs\"]]\n",
    "        token.tag = tag.text\n",
    "        \n",
    "    # lex-Dictionary enthält für jede Token-ID: word, id, pos-tag, lemma\n",
    "\n",
    "    #3. Iterieren über sentences-Knoten und satzweises Erzeugen von \"Token/Tag\"- bzw. \"Token/Lemma\"-Strings: \n",
    "    # (Lookup in lex-Dictionary über tokenIDs)\n",
    "    #(in Abhängigkeit von argparse-Option -t = tags bei Ausführung):\n",
    "    for sent in root.find('tc:TextCorpus', ns).find('tc:sentences', ns):\n",
    "        if tags:\n",
    "            print(\" \".join([lex[id].word + \"/\" + lex[id].tag for id in sent.attrib[\"tokenIDs\"].split(\" \")]))\n",
    "        else:\n",
    "            print(\" \".join([lex[id].word + \"/\" + lex[id].lemma for id in sent.attrib[\"tokenIDs\"].split(\" \")]))\n",
    "\n",
    "\n",
    "# Argumente für Programmaufruf über Kommandozeile (inklusive Help-Option):\n",
    "parser = argparse.ArgumentParser(description='write tokens with their tags or lemmas from tcf files')\n",
    "parser.add_argument('-t', '--tags', help='output tokens with their tag instead of their lemma', action=\"store_true\")\n",
    "parser.add_argument('urls', help='list of urls to operate on', nargs='*')\n",
    "\n",
    "args = parser.parse_args()\n",
    "for url in args.urls:\n",
    "    parse_xml(url, args.tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: xmltcf.py [-h] [-t] [urls [urls ...]]\n",
      "\n",
      "write tokens with their tags or lemmas from tcf files\n",
      "\n",
      "positional arguments:\n",
      "  urls        list of urls to operate on\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help  show this help message and exit\n",
      "  -t, --tags  output tokens with their tag instead of their lemma\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./xmltcf.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ausgabe Sätze mit POS-Tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./xmltcf.py -t https://www.deutschestextarchiv.de/book/download_fulltcf/16299 > sents.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIE/ART ELEMENTARORGANISMEN/NN UND/KON IHRE/ADJA BEZIEHUNGEN/NN ZU/APPR DEN/ART ZELLEN/NN ./$. VON/NN RICHARD/NE ALTMANN/NE ./$.\n",
      "MIT/NE ZWEI/ADJA ABBILDUNGEN/NN IM/APPR TEXT/NE UND/KON XXI/NE TAFELN./NE LEIPZIG/NE ,/$, VERLAG/NN VON/NE VEIT/NE &/$( COMP./NE 1890/CARD ./$.\n",
      "DIE/ART ELEMENTARORGANISMEN/NN UND/KON IHRE/ADJA BEZIEHUNGEN/NN ZU/APPR DEN/ART ZELLEN/NN ./$. VON/NN RICHARD/NE ALTMANN/NE ./$.\n",
      "MIT/NE ZWEI/ADJA ABBILDUNGEN/NN IM/APPR TEXT/NE UND/KON XXI/NE TAFELN./NE LEIPZIG/NE ,/$, VERLAG/NN VON/NE VEIT/NE &/$( COMP./NE 1890/CARD ./$.\n",
      "HERRN/NN WILHELM/NE HIS/NE IN/APPR DANKBARER/ADJA VEREHRUNG/NN GEWIDMET/VVPP VOM/ADJA VERFASSER/NN ./$.\n",
      "Vorbemerkung/NN ./$.\n",
      "Die/ART nachfolgenden/ADJA Capitel/NN enthalten/VVFIN im/APPRART Wesentlichen/NN eine/ART theils/ADV erweiterte/ADJA ,/$, theils/ADV verkürzte/ADJA Zusammenstellung/NN derjenigen/PDS Abhandlungen/NN ,/$, welche/PRELS bisher/ADV von/APPR mir/PPER über/APPR die/ART Zellengranula/NN veröffentlicht/VVPP worden/VAPP sind/VAFIN ./$.\n",
      "Indem/KOUS hierzu/PAV die/ART Beschreibung/NN der/ART Methoden/NN und/KON die/ART erläuternden/ADJA Abbildungen/NN kommen/VVFIN ,/$, dürfte/VMFIN das/ART Ganze/NN wohl/ADV geeignet/VVPP sein/VAINF ,/$, den/ART jetzigen/ADJA Standpunkt/NN der/ART Granulafrage/NN zu/PTKZU zeigen/VVINF ./$.\n",
      "So/ADV unvollkommen/ADJD dieser/PDAT Standpunkt/NN auch/ADV noch/ADV sein/VAINF mag/VMFIN ,/$, so/ADV liegt/VVFIN wohl/ADV immerhin/ADV schon/ADV ein/ART genügendes/ADJA Material/NN vor/PTKVZ ,/$, um/KOUI das/ART Geschick/NN jener/PDAT Lehre/NN für/APPR die/ART Zukunft/NN zu/APPR sichern/ADJA ./$.\n",
      "Das/ART Bewusstsein/NN ,/$, dass/KOUS uns/PPER hier/ADV die/ART Grundprobleme/NN der/ART Biologie/NN berühren/VVINF ,/$, wird/VAFIN es/PPER hoffentlich/ADJD herbeiführen/VVINF ,/$, dass/KOUS jener/PDAT Frage/NN sachliche/ADJA Mitarbeiter/NN gewonnen/VVPP werden/VAINF ,/$, denn/KON die/ART Kraft/NN des/ART Einzelnen/NN ist/VAFIN zu/PTKA gering/ADJD ,/$, um/KOUI den/ART hier/ADV vorhandenen/ADJA Anforderungen/NN zu/PTKZU genügen/VVINF ./$.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sed 10q sents.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ausgabe Sätze mit Lemmata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "./xmltcf.py https://www.deutschestextarchiv.de/book/download_fulltcf/16299 > sents_lemmata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIE/d ELEMENTARORGANISMEN/Elementarorganismus UND/und IHRE/Ihre BEZIEHUNGEN/Beziehung ZU/zu DEN/d ZELLEN/Zelle ./. VON/VON RICHARD/Richard ALTMANN/Altmann ./.\n",
      "MIT/Massachussetts_Institute_Of_Technology ZWEI/zwei ABBILDUNGEN/Abbildung IM/IM TEXT/Text UND/und XXI/Xxi TAFELN./TAFELN. LEIPZIG/Leipzig ,/, VERLAG/Verlag VON/VON VEIT/Veit &/& COMP./COMP. 1890/1890 ./.\n",
      "DIE/d ELEMENTARORGANISMEN/Elementarorganismus UND/und IHRE/Ihre BEZIEHUNGEN/Beziehung ZU/zu DEN/d ZELLEN/Zelle ./. VON/VON RICHARD/Richard ALTMANN/Altmann ./.\n",
      "MIT/Massachussetts_Institute_Of_Technology ZWEI/zwei ABBILDUNGEN/Abbildung IM/IM TEXT/Text UND/und XXI/Xxi TAFELN./TAFELN. LEIPZIG/Leipzig ,/, VERLAG/Verlag VON/VON VEIT/Veit &/& COMP./COMP. 1890/1890 ./.\n",
      "HERRN/Herr WILHELM/Wilhelm HIS/His IN/in DANKBARER/dankbar VEREHRUNG/Verehrung GEWIDMET/widmen VOM/vom VERFASSER/Verfasser ./.\n",
      "Vorbemerkung/Vorbemerkung ./.\n",
      "Die/d nachfolgenden/nachfolgend Capitel/Kapitel enthalten/enthalten im/im Wesentlichen/Wesentliche eine/eine theils/teils erweiterte/erweitert ,/, theils/teils verkürzte/verkürzt Zusammenstellung/Zusammenstellung derjenigen/diejenige Abhandlungen/Abhandlung ,/, welche/welche bisher/bisher von/von mir/ich über/über die/d Zellengranula/Zellengranulum veröffentlicht/veröffentlichen worden/werden sind/sein ./.\n",
      "Indem/indem hierzu/hierzu die/d Beschreibung/Beschreibung der/d Methoden/Methode und/und die/d erläuternden/erläuternd Abbildungen/Abbildung kommen/kommen ,/, dürfte/dürfen das/d Ganze/Ganze wohl/wohl geeignet/eignen sein/sein ,/, den/d jetzigen/jetzig Standpunkt/Standpunkt der/d Granulafrage/Granulumfrage zu/zu zeigen/zeigen ./.\n",
      "So/so unvollkommen/unvollkommen dieser/diese Standpunkt/Standpunkt auch/auch noch/noch sein/sein mag/mögen ,/, so/so liegt/liegen wohl/wohl immerhin/immerhin schon/schon ein/eine genügendes/genügend Material/Material vor/vor ,/, um/um das/d Geschick/Geschick jener/jen Lehre/Lehre für/für die/d Zukunft/Zukunft zu/zu sichern/sicher ./.\n",
      "Das/d Bewusstsein/Bewußtsein ,/, dass/dass uns/wir hier/hier die/d Grundprobleme/Grundproblem der/d Biologie/Biologie berühren/berühren ,/, wird/werden es/es hoffentlich/hoffentlich herbeiführen/herbeiführen ,/, dass/dass jener/jen Frage/Frage sachliche/sachlich Mitarbeiter/Mitarbeiter gewonnen/gewinnen werden/werden ,/, denn/denn die/d Kraft/Kraft des/d Einzelnen/Einzelne ist/sein zu/zu gering/gering ,/, um/um den/d hier/hier vorhandenen/vorhanden Anforderungen/Anforderung zu/zu genügen/genügen ./.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sed 10q sents_lemmata.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
